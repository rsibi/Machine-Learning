{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_table('/home/sibi/acad/mach_learn_with_networks/assignment/2_lm_opt/bclass/bclass/bclass-train',delimiter='\\t',header=None)\n",
    "df_test = pd.read_table('/home/sibi/acad/mach_learn_with_networks/assignment/2_lm_opt/bclass/bclass/bclass-test',delimiter='\\t',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Converting -1 to 0\n",
    "df_train.head()\n",
    "df_train.loc[df_train[0] == -1,0] = 0\n",
    "df_test.loc[df_test[0] == -1,0] = 0\n",
    "df_train = df_train.drop(2, 1) #CAREFUL\n",
    "df_test = df_test.drop(2,1) #CAREFUL LABEL NAMES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting as features and labels\n",
    "X_train = df_train.ix[:,1:]\n",
    "y_train = df_train[0]\n",
    "X_test = df_test.loc[:,1:]\n",
    "y_test = df_test.loc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally weighted logistic regression\n",
    "\n",
    "## Gradient Descent Function\n",
    "\n",
    "The locally weighted logistic regression problem is to maximize\n",
    "$$ l(\\theta) = \\sum_{i=1}^{N} w^i \\{y^ilog\\:f_\\beta (x^i) + (1-y^i)\\:log(1 - log\\:f_\\beta(x^i)]\\} - \\lambda\\beta^T\\beta$$\n",
    "\n",
    "We first solve for the gradient and hessian using this function l. We can then conver this as an iterative procedure using the Newton-Raphson method.\n",
    "\n",
    "The gradient is given by  \n",
    "\n",
    "$\\quad \\nabla_\\theta\\:l(\\theta) = X^Tz - \\lambda\\theta$\n",
    "\n",
    "where z is the expresion \n",
    "\n",
    "$\\quad z_i = w^i \\:(y^i - h_\\theta(x^i)\\:)$\n",
    "\n",
    "and the Hessian is given by \n",
    "\n",
    "$\\quad H = X^TDX - \\lambda I$\n",
    "\n",
    "Where D is the diagonal matrix\n",
    "\n",
    "$\\quad D_{ii} = -w^i h_\\theta(x^i)(1-h_\\theta(x^i))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lambda_value = 0.001\n",
    "\n",
    "def sigmoid(z): \n",
    "    return 1 / (1 + np.e**(-z)) \n",
    "\n",
    "def hypothesis(theta, x):\n",
    "    return sigmoid(np.dot(x, theta))\n",
    "\n",
    "def gaussian(point1, point2, tau):\n",
    "        return np.exp(- (np.linalg.norm(a-b)**2) / (2 * tau**2))\n",
    "\n",
    "def getWeights(data, new_point, tau):\n",
    "    assert (new_point.shape[0] == 1) #new point should be 1xn, where n is the number of features\n",
    "    new_point_repeat = np.repeat(new_point, data.shape[0], axis = 0) #for a 2-D array, axis=0 == rows, axis=1 == columns    \n",
    "    norm_squared = np.linalg.norm(data - new_point_repeat, axis = 1) ** 2\n",
    "    weights = np.exp(-norm_squared / (2 * tau ** 2))\n",
    "    return weights.reshape(len(weights), 1)\n",
    "    \n",
    "def lw_logreg(x_train, y_train, x, tau):\n",
    "    x_train_aug = pd.concat([pd.Series(np.ones(len(x_train.index))), x_train], axis = 1)\n",
    "    x_aug = np.concatenate([np.asarray([[1]]), x], axis = 1)\n",
    "    \n",
    "    theta_length = x_train_aug.shape[1] \n",
    "    theta = np.zeros([theta_length, 1])\n",
    "    gradient = np.ones([33,1])\n",
    "\n",
    "    dist = 1\n",
    "    i = 0\n",
    "    while dist > 0.0001 and i < 100:\n",
    "        weights = getWeights(x_train_aug, x_aug, tau)\n",
    "#         print hypothesis(theta,x_train_aug).shape\n",
    "#         print weights.shape\n",
    "#         print y_train.shape\n",
    "        y_train = y_train.reshape(200,1)\n",
    "        z = weights * (y_train - hypothesis(theta, x_train_aug))\n",
    "        gradient = np.dot(x_train_aug.transpose(), z) -  LAMBDA * theta\n",
    "\n",
    "        diags = -weights * hypothesis(theta, x_aug) * (1 - hypothesis(theta, x_aug))\n",
    "        D = np.diag(diags[:, 0])\n",
    "        H = np.dot(np.dot(x_train_aug.transpose(), D), x_train_aug) - LAMBDA * np.eye(x_train_aug.shape[1])\n",
    "\n",
    "        theta_new = theta - np.dot(np.linalg.inv(H), gradient)\n",
    "\n",
    "        dist = np.linalg.norm(theta_new - theta)\n",
    "        theta = theta_new\n",
    "        i +=1\n",
    "\n",
    "    return theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "tau = 0.1\n",
    "for i in xrange(200):\n",
    "    x = X_train.iloc[[i]]\n",
    "    final_theta = lw_logreg(X_train, y_train, x, tau)\n",
    "    x_aug = np.concatenate([np.asarray([[1]]), x], axis = 1)\n",
    "\n",
    "    if hypothesis(final_theta,x_aug) > 0.5:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07200779]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_train.iloc[[25]]\n",
    "x_aug = np.concatenate([np.asarray([[1]]), x], axis = 1)\n",
    "# print x_aug\n",
    "# print final_theta\n",
    "hypothesis(final_theta,x_aug)\n",
    "sigmoid(np.dot(x_aug, final_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "train_acc = metrics.accuracy_score(y_train,pred)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
